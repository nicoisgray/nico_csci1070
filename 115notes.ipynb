{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Warmup\n",
    "Bagging is the model that has the parts of the ensemble learning independently from one another, and combines them to get an average. Boosting has the parts of the ensemble learning from each other and affecting each others' outcomes based on the previous data. These methods reduce variance because they combine estimates from several weaker models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Spongebob', 'Patrick', 'Sandy', 'Mr. Krabs', 'Plankton']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def listswap (listname, a, b):\n",
    "    \"\"\"\n",
    "    swaps the position of the item at index a with the item at index b\n",
    "    \"\"\"\n",
    "    listname[a], listname[b] = listname[b], listname[a]\n",
    "    return listname\n",
    "\n",
    "characters = [\"Spongebob\", \"Patrick\", \"Plankton\", \"Mr. Krabs\", \"Sandy\"]\n",
    "listswap(characters, 2, 4)\n",
    "\n",
    "#generally you want to use return in your functions as a better way to end them, as opposed to using print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AUC and ROC\n",
    "What is an ROC curve? Answer: It is a graph that shows the performance of a classifier by plotting the true positive rate and false positive rate. It's better than just looking at the rates on their own because it displays the values visually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is an AUC curve? Answer: Area under curve shows your overall takeaway from your ROC curve. In other words, it shows performance of your classification problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Learning\n",
    "## Two main approaches: Clustering or Dimensionality Reduction\n",
    "Clustering = finding patterns in the data. We don't know the outcome. Includes k-means (using literal distance), hierarchical (finds how close two points are to each other) grid-based, density-based.\n",
    "Dimensionality reduction = reducing the dimensions of the data to what actually affects the outcome. Linear forms include principal component analysis (PCA), SVD for sparse data (like datasets with a lot of NaN missing values), linear discriminant. Nonlinear has isomap embedding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use unsupervised learning instead of labeling our data because labeling can be time-consuming. Sometimes, there is no answer to the type of problem and you just need to find patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering is a process that looks for inherent groupings in data.\n",
    "K-means first randomly assigns a mean value for each of the K clusters. Then, all the values are assigned to the closest mean. Finally, the means of the clusters are recalculated based on the actual values, and the process repeats until this mean is closest to the mean of the values in the cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hierarchical clustering takes a tree-based structure and groups data together based on proximity into repeatedly smaller groups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding the ideal number of clusters\n",
    "For k-means, we use the measure of inertia, elbow method, gap method, and/or average silhouette method. Elbow method uses curve of within-cluster sum of squares. Gap method tries to maximize the distance between clusters. The average silhouette method determines how well each value fits in each cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ideal clusters for hierarchical\n",
    "Find the maximum distance in the dendrogram. This represents the max distance (y-axis) between clusters. The number of clusters is how many clusters intersected each other at the max distance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionality reduction\n",
    "The goal is to reduce dimensions in the dataset. This is because models with a lot of dimensions are highly complex and are prone to overfitting. There are linear and non-linear ways of doing this."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
